{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-1.21.4-cp39-cp39-macosx_10_9_x86_64.whl (17.0 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.21.4\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "#from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# from gensim import corpora\n",
    "# from gensim.models.ldamodel import LdaModel\n",
    "import json, nltk, pickle, ast\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon review dataset specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> data[0] <br>\n",
    "    {<br>\n",
    "    'asin': '0528881469',<br>\n",
    "     'helpful': [0, 0],<br>\n",
    "     'overall': 5.0,<br>\n",
    "     'reviewText': 'We got this GPS for my husband who is an (OTR) over the road trucker.  Very Impressed with the shipping time, it arrived a few days earlier than expected...  within a week of use however it started freezing up... could of just been a glitch in that unit.  Worked great when it worked!  Will work great for the normal person as well but does have the \"trucker\" option. (the big truck routes - tells you when a scale is coming up ect...)  Love the bigger screen, the ease of use, the ease of putting addresses into memory.  Nothing really bad to say about the unit with the exception of it freezing which is probably one in a million and that\\'s just my luck.  I contacted the seller and within minutes of my email I received a email back with instructions for an exchange! VERY impressed all the way around!',<br>\n",
    "     'reviewTime': '06 2, 2013',<br>\n",
    "     'reviewerID': 'AO94DHGC771SJ',<br>\n",
    "     'reviewerName': 'amazdnu',<br>\n",
    "     'summary': 'Gotta have GPS!',<br>\n",
    "     'unixReviewTime': 1370131200<br>\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct dataset with LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_dir: ./data/electronics/with_lda/\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'electronics'\n",
    "file_name = '{}_simple_json.json'.format(dataset_name)\n",
    "data = json.load(open('./raw_data/{}'.format(file_name), 'r'))\n",
    "\n",
    "num_aspects = 30\n",
    "num_top_words = 5\n",
    "random_state_lda = 42\n",
    "save_dir = './data/{}/with_lda/'.format(dataset_name)\n",
    "\n",
    "users = list(set([ r['reviewerID'] for r in data ]))\n",
    "items = list(set([ r['asin'] for r in data ]))\n",
    "\n",
    "'''\n",
    "    :Tokenize reviews into lists of sentences (which are lists of tokens)`\n",
    "    :param sentences - sentences from a review\n",
    "    :return 2d lists (of sentences of tokens)\n",
    "'''\n",
    "def tokenize(reviews):\n",
    "    tokenized_ss = []\n",
    "    # make reviews lowercase, store tokens per sentence\n",
    "    for r in data:\n",
    "        r_text = r['reviewText']\n",
    "        r_user = r['reviewerID']\n",
    "        r_item = r['asin']\n",
    "\n",
    "        sentences = sent_tokenize(r_text.lower()) \n",
    "    for s in sentences:\n",
    "        tokenized_s = word_tokenize(s)\n",
    "        tokenized_ss.append(['<sos>'] + tokenized_s + ['<eos>'])\n",
    "    \n",
    "    return tokenized_ss\n",
    "\n",
    "'''\n",
    "    :Extract topics with LDA\n",
    "    :param - data (amazon review dataset)\n",
    "    :return topic_info (e.g., [(TOPIC-IDX-0, [('early', 0.0029411765), ('apps', 0.0029411765), ... ]), (TOPIC-IDX-1, ...), ...]\n",
    "'''\n",
    "def extract_topics(data):\n",
    "    tokenized_ss = tokenize(data)\n",
    "    \n",
    "    # turn it into frequency matrix\n",
    "    idx2w_dict = corpora.Dictionary(tokenized_ss) # e.g., { ..., 980: 'pleased', 981: 'purpose', ...}\n",
    "    w2idx_dict = idx2w_dict.token2id\n",
    "    s2w_matrix = [idx2w_dict.doc2bow(s_tokens) for s_tokens in tokenized_ss]\n",
    "\n",
    "    ldamodel = LdaModel(s2w_matrix, num_topics = num_aspects, id2word=idx2w_dict, passes=15)\n",
    "    \n",
    "    # For each sentence & each word in sentences, get topic probability of sentences and words\n",
    "    for tokenized_s in tokenized_ss:\n",
    "        doc_topics, word_topics, _ = ldamodel.get_document_topics(idx2w_dict.doc2bow(tokenized_s), per_word_topics=True)\n",
    "\n",
    "    # For each topic, get the top-100 words\n",
    "    topic_info = ldamodel.show_topics(num_topics=12, num_words=100, formatted=False)\n",
    "    \n",
    "    return topic_info\n",
    "\n",
    "'''\n",
    "    :Build topic2idx, idx2topic\n",
    "'''\n",
    "def build_topic2idx_idx2topic_from_topic(topics):\n",
    "    topic2words = {}\n",
    "    word2tIdx = {}  # for topic2idx\n",
    "    tIdx2word = {} # for idx2topic\n",
    "    \n",
    "    topic2words = { t_info[0]:[ t_words[0] for t_words in t_info[1]] for t_info in topics }\n",
    "    for t_id, words in topic2words.items():\n",
    "        for word in words:\n",
    "            if word not in word2tIdx.keys():\n",
    "                word2tIdx[word] = []\n",
    "            word2tIdx[word] = t_id\n",
    "\n",
    "    tIdx2word = { t_id:w for w, t_id in zip(word2tIdx.keys(), word2tIdx.values()) }\n",
    "    \n",
    "    return word2tIdx, tIdx2word\n",
    "\n",
    "topics = extract_topics(data)\n",
    "word2tIdx, tIdx2word = build_topic2idx_idx2topic_from_topic(topics)\n",
    "\n",
    "user2idx = {}\n",
    "item2idx = {}\n",
    "\n",
    "# users to their idx\n",
    "u_ids = list(set([ r['reviewerID'] for r in data ]))\n",
    "user2idx = { u_id:idx for idx, u_id in enumerate(u_ids) }\n",
    "user2idx_rev = { v:k for k, v in user2idx.items() }\n",
    "\n",
    "i_ids = list(set([ r['asin'] for r in data ]))\n",
    "item2idx = { i_id:idx for idx, i_id in enumerate(i_ids) }\n",
    "item2idx_rev = { idx:i_id for idx, i_id in enumerate(i_ids) }\n",
    "\n",
    "print('save_dir: {}'.format(save_dir))\n",
    "pickle.dump(word2tIdx, open(save_dir + 'aspect2idx.pkl', 'wb'))\n",
    "pickle.dump(tIdx2word, open(save_dir + 'idx2aspect.pkl', 'wb'))\n",
    "pickle.dump(user2idx, open(save_dir + 'user2idx.pkl', 'wb'))\n",
    "pickle.dump(user2idx_rev, open(save_dir + 'idx2user.pkl', 'wb'))\n",
    "pickle.dump(item2idx, open(save_dir + 'item2idx.pkl', 'wb'))\n",
    "pickle.dump(item2idx_rev, open(save_dir + 'idx2item.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing data from extracted aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_dir: ./data/electronics/\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'electronics'\n",
    "file_name = '{}_with_aspects.pkl'.format(dataset_name)\n",
    "data = pickle.load(open('./raw_data/{}'.format(file_name), 'rb'))\n",
    "save_dir = './data/{}/'.format(dataset_name)\n",
    "\n",
    "# Go over each review\n",
    "'''\n",
    "[{'rating': 4,\n",
    "  'item': 'B000HJ75MK',\n",
    "  'user': 'A37021SSH9LGY6',\n",
    "  'sentence': [('mouse',\n",
    "    'light',\n",
    "    'lightweight mouse to take on a trip overseas',\n",
    "    1)],\n",
    "  'text': 'Mighty little mouse\\nI ... '\n",
    "  }, ... \n",
    "]\n",
    "# sentence: [(ASPECT, SENTIMENT-WORD, RELATED-SENTENCE, SENTIMENT-SCORE), ...]\n",
    "'''\n",
    "\n",
    "df_data = pd.DataFrame(data)\n",
    "df_data_flatten = df_data.explode('sentence')\n",
    "\n",
    "# Extract aspects from a['sentence'][0]\n",
    "aspects = set([ a['sentence'][0] if not pd.isna(a['sentence']) else None for i, a in df_data_flatten.iterrows() ])\n",
    "\n",
    "aspect2idx = { a:idx for idx, a in enumerate(aspects) }\n",
    "idx2aspect = { idx:a for idx, a in enumerate(aspects) }\n",
    "\n",
    "u_ids = list(df_data['user'].unique())\n",
    "user2idx = { u:idx for idx, u in enumerate(u_ids)}\n",
    "idx2user = { idx:u for idx, u in enumerate(u_ids)}\n",
    "             \n",
    "i_ids = list(df_data['item'].unique())\n",
    "item2idx = { i:idx for idx, i in enumerate(i_ids)}\n",
    "idx2item = { idx:i for idx, i in enumerate(i_ids)}\n",
    "\n",
    "print('save_dir: {}'.format(save_dir))\n",
    "pickle.dump(aspect2idx, open(save_dir + 'aspect2idx.pkl', 'wb'))\n",
    "pickle.dump(idx2aspect, open(save_dir + 'idx2aspect.pkl', 'wb'))\n",
    "pickle.dump(user2idx, open(save_dir + 'user2idx.pkl', 'wb'))\n",
    "pickle.dump(idx2user, open(save_dir + 'idx2user.pkl', 'wb'))\n",
    "pickle.dump(item2idx, open(save_dir + 'item2idx.pkl', 'wb'))\n",
    "pickle.dump(idx2item, open(save_dir + 'idx2item.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82753"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_ids = list(df_data['user'].unique())\n",
    "user2idx = { u:i for i, u in enumerate(u_ids)}\n",
    "idx2user = { i:u for i, u in enumerate(u_ids)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "      <th>user</th>\n",
       "      <th>sentence</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35700</th>\n",
       "      <td>Great SD card-FAST!\\nPurchased one of these la...</td>\n",
       "      <td>5</td>\n",
       "      <td>ALM9R3A4ZGFHC</td>\n",
       "      <td>[(cost, next, Wonder what a 32G card will cost...</td>\n",
       "      <td>B002TA7VO2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35701</th>\n",
       "      <td>For my wifes Canon camera\\nShe loves it and us...</td>\n",
       "      <td>3</td>\n",
       "      <td>A3VL6OOZVAA9OR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B002TA7VO2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35702</th>\n",
       "      <td>Works fine on Windows 7 and in my MP3 player\\n...</td>\n",
       "      <td>5</td>\n",
       "      <td>A1DQIKMPS16H54</td>\n",
       "      <td>[(card, perfect, For this kind of purpose the ...</td>\n",
       "      <td>B002TA7VO2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35703</th>\n",
       "      <td>Works very well.\\nThis item works very well in...</td>\n",
       "      <td>5</td>\n",
       "      <td>ASD69N2P9WRSU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B002TA7VO2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35704</th>\n",
       "      <td>Does the expected.\\nDoes the expected. Writes ...</td>\n",
       "      <td>3</td>\n",
       "      <td>A1KMB1WVG8QM8H</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B002TA7VO2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35705</th>\n",
       "      <td>Not the fastest, but surely a good price for t...</td>\n",
       "      <td>5</td>\n",
       "      <td>AVBHYYARHGNKF</td>\n",
       "      <td>[(price, good, but surely a good price for the...</td>\n",
       "      <td>B002TA7VO2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35706</th>\n",
       "      <td>It's SDHC memory\\nIts SDHC memory. Bought it t...</td>\n",
       "      <td>5</td>\n",
       "      <td>A21G8XF090VXN4</td>\n",
       "      <td>[(camera, digital, Bought it to go in a digita...</td>\n",
       "      <td>B002TA7VO2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  rating  \\\n",
       "35700  Great SD card-FAST!\\nPurchased one of these la...       5   \n",
       "35701  For my wifes Canon camera\\nShe loves it and us...       3   \n",
       "35702  Works fine on Windows 7 and in my MP3 player\\n...       5   \n",
       "35703  Works very well.\\nThis item works very well in...       5   \n",
       "35704  Does the expected.\\nDoes the expected. Writes ...       3   \n",
       "35705  Not the fastest, but surely a good price for t...       5   \n",
       "35706  It's SDHC memory\\nIts SDHC memory. Bought it t...       5   \n",
       "\n",
       "                 user                                           sentence  \\\n",
       "35700   ALM9R3A4ZGFHC  [(cost, next, Wonder what a 32G card will cost...   \n",
       "35701  A3VL6OOZVAA9OR                                                NaN   \n",
       "35702  A1DQIKMPS16H54  [(card, perfect, For this kind of purpose the ...   \n",
       "35703   ASD69N2P9WRSU                                                NaN   \n",
       "35704  A1KMB1WVG8QM8H                                                NaN   \n",
       "35705   AVBHYYARHGNKF  [(price, good, but surely a good price for the...   \n",
       "35706  A21G8XF090VXN4  [(camera, digital, Bought it to go in a digita...   \n",
       "\n",
       "             item  \n",
       "35700  B002TA7VO2  \n",
       "35701  B002TA7VO2  \n",
       "35702  B002TA7VO2  \n",
       "35703  B002TA7VO2  \n",
       "35704  B002TA7VO2  \n",
       "35705  B002TA7VO2  \n",
       "35706  B002TA7VO2  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data[df_data['item'].str.contains('B002TA7VO2')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing user, item.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "### constructing user2idx, item2idx\n",
    "### (user/item to ids of tokens from their reviews)\n",
    "# for r in data:\n",
    "#     r_text = r['reviewText']\n",
    "#     r_user = r['reviewerID']\n",
    "#     r_item = r['asin']\n",
    "#     tokenized_r = word_tokenize(r_text.lower())\n",
    "    \n",
    "#     if r_user not in user2idx.keys():\n",
    "#         user2idx[r_user] = []\n",
    "        \n",
    "#     if r_item not in item2idx.keys():\n",
    "#         item2idx[r_item] = []\n",
    "    \n",
    "#     for token in tokenized_r:\n",
    "#         w_idx = w2idx_dict[token]\n",
    "#         user2idx[r_user].append(w_idx)\n",
    "#         item2idx[r_item].append(w_idx)\n",
    "\n",
    "\n",
    "# # only contains unique set of words by removing duplicates\n",
    "# for u_id in list(user2idx.keys()):\n",
    "#     unique_w_ids = list(set(user2idx[u_id]))\n",
    "#     if len(unique_w_ids) < num_top_words:\n",
    "#         del user2idx[u_id]\n",
    "#         print('user {} deleted'.format(u_id))\n",
    "#     else:\n",
    "#         user2idx[u_id] = unique_w_ids[:num_top_words]\n",
    "    \n",
    "# for i_id in list(item2idx.keys()):\n",
    "#     unique_w_ids = list(set(item2idx[i_id]))\n",
    "#     if len(unique_w_ids) < num_top_words:\n",
    "#         del item2idx[i_id]\n",
    "#         print('item {} deleted'.format(i_id))\n",
    "#     else:\n",
    "#         item2idx[i_id] = unique_w_ids[:num_top_words]\n",
    "        \n",
    "# pickle.dump(user2idx, open(save_dir + 'user.pkl', 'wb'))\n",
    "# pickle.dump(item2idx, open(save_dir + 'item.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-47-661e3c02d3e6>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-47-661e3c02d3e6>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    but the topic is per sentence...\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    a u/i review -- a sequence of words which are assigned to one of topics each\n",
    "    // e.g., \n",
    "    \"topic.pkl\": topic2idx dictionary, including <sos>, <eos>, <unk>, <pad>, and topic labels. \n",
    "    // {TOPIC-IDX: (TOPIC-WORD-IDX1, TOPIC-WORD-IDX2, ...)} e.g., {0: (1,1002,1123, ...), }\n",
    "    \"topic_rev.pkl\": idx2topic dictionary, the reverse of topic2idx\n",
    "    \"user.pkl\" and \"item.pkl\": user2idx and item2idx dictionary\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train/val/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "random_state = 42\n",
    "file_to_split = 'electronics_with_aspects'\n",
    "\n",
    "# Load original review dataset (100k sampled)\n",
    "# Dataset is not really jsonized one -- json elements are line-separated\n",
    "# with open('./raw_data/{}.json'.format(file_to_split), 'r') as f:\n",
    "#     all_data = [ eval(l) for l in f.readlines() ]\n",
    "    \n",
    "# Load files for extracted aspects\n",
    "all_data = json.load(open('./raw_data/{}.json'.format(file_to_split), 'r'))\n",
    "    \n",
    "train_ratio = 0.8\n",
    "validation_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "x_train, x_test = train_test_split(all_data, test_size=1 - train_ratio, random_state=random_state)\n",
    "x_val, x_test = train_test_split(x_test, test_size=test_ratio/(test_ratio + validation_ratio), random_state=random_state)\n",
    "\n",
    "# Save splitted train/test/valid jsons back to file\n",
    "with open('./data/electronics/train.json', 'w') as f:\n",
    "    for r_dict in x_train:\n",
    "        f.write(json.dumps(r_dict))\n",
    "        f.write('\\n')\n",
    "    f.close()\n",
    "    \n",
    "with open('./data/electronics/test.json', 'w') as f:\n",
    "    for r_dict in x_test:\n",
    "        f.write(json.dumps(r_dict))\n",
    "        f.write('\\n')\n",
    "    f.close()\n",
    "    \n",
    "with open('./data/electronics/validation.json', 'w') as f:\n",
    "    for r_dict in x_val:\n",
    "        f.write(json.dumps(r_dict))\n",
    "        f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = json.load(open('./raw_data/{}.json'.format(file_to_split), 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118312"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda)",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
